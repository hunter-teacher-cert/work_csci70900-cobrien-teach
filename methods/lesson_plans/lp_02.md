# lesson plan \# 2 (CS methods)

## context
This class immediately follows the previous one described in lp_01.md. See that lesson plan for details.

One issue I struggled with was how much coding students should actually do for this lesson.  I decided to focus less on coding and more on the big ideas, since the goal is to introduce students to big picture questions of linguistics, machine learning, etc., not to train NLP engineers.

## aim
Understand  how bigram and unigram Markov models are implemented in Python to generate English sentences.
## do now (8 min.)
* Present Weather Markov chain on Board (see weather_markov.png). Questions for students:

  * explain in your own words what this diagram represents

  * What questions do you have about this diagram?

* Students share in pairs, followed by group share out.



## instruction (15 min.)
* Walk through how unigram.py works, explaining each step.

  * Input a long book (e.g. War & peace). Generate a few sentences. Some of these are weird.

* Introduce "bigrams". Briefly model for same sentence as yesterday "I hope to tell a joke to Maria".

  * Show bigram.py. Generate a few sentences.

## Group activity (15 min).
* Students are in groups (They have access to code for). Discuss and answer the following questions as a group:

  * What are the similarities between unigram.py and bigram.py? The differences?

  * Do you think these programs 'know' English? Why or why not?

## Whole group discussion:

* person from each group shares out.

* Discussion: What are your reactions to other group's comments?
